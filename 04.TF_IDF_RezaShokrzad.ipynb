{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OByqxxRfPokl"
   },
   "source": [
    "# Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "* TF-IDF is a **statistical** measure.\n",
    "* It reflects how important/relevant a word is to a document in a collection or corpus.\n",
    "* It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.\n",
    "* A [survey conducted in 2015](https://kops.uni-konstanz.de/handle/123456789/32348) showed that 83% of text-based recommender systems in digital libraries use tf–idf.\n",
    "* The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.\n",
    "* Applications: Search Engines (in determining the relevance of queries and documents) and stop-words removal (especially in text-summarization or document classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGS2ROQIXbmZ"
   },
   "source": [
    "## Term Frequency (TF)\n",
    "* The first form of term weighting is due to Hans Peter Luhn (1957) [link text](https://ieeexplore.ieee.org/document/5392697)\n",
    "* The number of times a term occurs in a document is called its term frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypqPjtYcxTli"
   },
   "source": [
    "## Inverse Document Frequency\n",
    "* Motivation: TF will tend to incorrectly emphasize documents which happen to use the words like \"the\" more frequently\n",
    "* [Karen Spärck Jones](https://en.wikipedia.org/wiki/Karen_Sp%C3%A4rck_Jones) (1972) conceived a statistical interpretation of term-specificity called Inverse Document Frequency (idf), which became a cornerstone of term weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scipy scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "BFwFM6TITebv"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import lil_matrix\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "lIYdn1woOS1n"
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'this is the first document',\n",
    "    'this document is the second document',\n",
    "    'and this is the third one',\n",
    "    'is this the first document',\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0Dq2baGpSYTb"
   },
   "outputs": [],
   "source": [
    "def IDF(corpus, unique_words):\n",
    "   idf_dict={}\n",
    "   N=len(corpus)\n",
    "   for i in unique_words:\n",
    "     count=0\n",
    "     for sen in corpus:\n",
    "       if i in sen.split():\n",
    "         count=count+1\n",
    "       idf_dict[i]=(math.log((1+N)/(count+1)))+1\n",
    "   return idf_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "LF6z9dvpSaEv"
   },
   "outputs": [],
   "source": [
    "def fit(whole_data):\n",
    "    unique_words = set()\n",
    "    if isinstance(whole_data, (list,)):\n",
    "      for x in whole_data:\n",
    "        for y in x.split():\n",
    "          if len(y)<2:\n",
    "            continue\n",
    "          unique_words.add(y)\n",
    "      unique_words = sorted(list(unique_words))\n",
    "      vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "      Idf_values_of_all_unique_words=IDF(whole_data,unique_words)\n",
    "    return vocab, Idf_values_of_all_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "s8X_WuYl3fOI"
   },
   "outputs": [],
   "source": [
    "Vocabulary, idf_of_vocabulary=fit(corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8bjRArb51TF_",
    "outputId": "4439137b-74b8-4435-81c8-df2135d6b418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(list(Vocabulary.keys())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Dy8KSkm31Z8V"
   },
   "outputs": [],
   "source": [
    "def transform(dataset, vocabulary, idf_values):\n",
    "    sparse_matrix = lil_matrix((len(dataset), len(vocabulary)), dtype=np.float64)\n",
    "    \n",
    "    for row in range(0, len(dataset)):\n",
    "        words = dataset[row].split()\n",
    "        number_of_words_in_sentence = Counter(words)\n",
    "        sentence_length = len(words)\n",
    "        \n",
    "        for word in words:\n",
    "            if word in vocabulary:\n",
    "                tf = number_of_words_in_sentence[word] / sentence_length\n",
    "                tf_idf_value = tf * idf_values[word]\n",
    "                sparse_matrix[row, vocabulary[word]] = tf_idf_value\n",
    "    \n",
    "    # Convert to csr_matrix for efficient operations\n",
    "    sparse_matrix = sparse_matrix.tocsr()\n",
    "    \n",
    "    print(\"VOCABULARY:\")\n",
    "    for word, idx in sorted(vocabulary.items()):\n",
    "        print(f\"  '{word}': {idx}\")\n",
    "    \n",
    "    print(\"\\nTF-IDF MATRIX (before normalization):\")\n",
    "    print(sparse_matrix.toarray())\n",
    "    \n",
    "    normalized_matrix = normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False)\n",
    "    \n",
    "    print(\"\\nTF-IDF MATRIX (after L2 normalization):\")\n",
    "    print(normalized_matrix.toarray())\n",
    "    \n",
    "    return normalized_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7IopUTyh1g6u",
    "outputId": "502ab4d7-56bf-4ce3-c957-74dd450be043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCABULARY:\n",
      "  'and': 0\n",
      "  'document': 1\n",
      "  'first': 2\n",
      "  'is': 3\n",
      "  'one': 4\n",
      "  'second': 5\n",
      "  'the': 6\n",
      "  'third': 7\n",
      "  'this': 8\n",
      "\n",
      "TF-IDF MATRIX (before normalization):\n",
      "[[0.         0.24462871 0.30216512 0.2        0.         0.\n",
      "  0.2        0.         0.2       ]\n",
      " [0.         0.40771452 0.         0.16666667 0.         0.31938179\n",
      "  0.16666667 0.         0.16666667]\n",
      " [0.31938179 0.         0.         0.16666667 0.31938179 0.\n",
      "  0.16666667 0.31938179 0.16666667]\n",
      " [0.         0.24462871 0.30216512 0.2        0.         0.\n",
      "  0.2        0.         0.2       ]]\n",
      "\n",
      "TF-IDF MATRIX (after L2 normalization):\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n",
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "final_output=transform(corpus,Vocabulary,idf_of_vocabulary)\n",
    "print(final_output.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dsWQhnPi1ipZ",
    "outputId": "9e0b6740-72bc-41d3-d501-ba3f227a631c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "print(final_output[0].toarray())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TF/IDF - RezaShokrzad",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
