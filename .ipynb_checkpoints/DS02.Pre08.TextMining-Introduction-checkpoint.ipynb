{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gabMsXl6nXFW"
      },
      "source": [
        "# Text Mining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnMk1pg3ojMS"
      },
      "source": [
        "# NLP\n",
        "Natural language processing (NLP) is a field located at the intersection of data science and Artificial Intelligence (AI) that – when boiled down to the basics – is all about teaching machines how to understand human languages and extract meaning from text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbGrvb8lqYem"
      },
      "source": [
        "## Applications:\n",
        "1. Document Classfication\n",
        "2. Review Analysis - Sentiment Analysis\n",
        "3. Search Engines\n",
        "4. Machine Translation\n",
        "5. Talker Bots\n",
        "6. Spell Correction\n",
        "7. Summarization\n",
        "8. Machine Conversation\n",
        "9. Spam Detection\n",
        "10. Name Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0QQj_NxooQX"
      },
      "source": [
        "## 8 best Python Natural Language Processing (NLP) libraries:\n",
        "1. **Natural Language Toolkit (NLTK):**\\\n",
        "https://www.nltk.org/ \\\n",
        "NLTK is an essential library supports tasks such as classification, stemming, tagging, parsing, semantic reasoning, and tokenization in Python. (Book: https://www.nltk.org/book/)\n",
        "\n",
        "2. **TextBlob:** \\\n",
        "https://textblob.readthedocs.io/en/dev/ \\\n",
        "TextBlob is a must for developers who are starting their journey with NLP in Python and want to make the most of their first encounter with NLTK.\n",
        "\n",
        "3. **CoreNLP:** \\\n",
        "https://stanfordnlp.github.io/CoreNLP/ \\\n",
        "This library was developed at Stanford University and it’s written in Java. Still, it’s equipped with wrappers for many different languages, including Python.\n",
        "\n",
        "4. **Gensim:** \\\n",
        "https://github.com/RaRe-Technologies/gensim \\\n",
        "Gensim is a Python library that specializes in identifying semantic similarity between two documents through vector space modeling and topic modeling toolkit.\n",
        "\n",
        "5. **spaCy:** \\\n",
        "https://spacy.io/ \\\n",
        "spaCy is a relatively young library was designed for production usage. That’s why it’s so much more accessible than other Python NLP libraries like NLTK.\n",
        "\n",
        "6. **polyglot:** \\\n",
        "https://polyglot.readthedocs.io/en/latest/index.html \\\n",
        "This slightly lesser-known library is one of our favorites because it offers a broad range of analysis and impressive language coverage. Thanks to NumPy, it also works really fast.\n",
        "\n",
        "7. **scikit-learn:** \\\n",
        "https://scikit-learn.org/ \\\n",
        "This handy NLP library provides developers with a wide range of algorithms for building machine learning models. It offers many functions for using the bag-of-words method of creating features to tackle text classification problems.\n",
        "\n",
        "8. **Pattern:** \\\n",
        "https://www.clips.uantwerpen.be/clips.bak/pages/pattern \\\n",
        "Another gem in the NLP libraries Python developers use to handle natural languages. Pattern allows part-of-speech tagging, sentiment analysis, vector space modeling, SVM, clustering, n-gram search, and WordNet. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygUOQDjWotKv"
      },
      "source": [
        "# Text Word-level Representation (Word Embedding)\n",
        "\n",
        "[Watch YouTube Videos for details](https://www.youtube.com/channel/UC3d1uzFtJxqPsirAc48zPEA) \\\n",
        "\n",
        "1. **One-hot Encoding:** \\\n",
        "A one hot encoding is a representation of categorical variables as binary vectors. Each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.\n",
        "2. **Bag-Of-Words:** \\\n",
        "In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.\n",
        "\n",
        "3. **Word-Embedding:** \\\n",
        "In the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.\n",
        "4. **TF-IDF:** \\\n",
        "This algorithm is widely used in the search technologies. Tf-Idf stands for Term frequency-Inverse document frequency.\n",
        "5. **Word2Vec:**\\\n",
        "The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WomB9KXgj0z"
      },
      "source": [
        "## NLTK\n",
        "\n",
        "[https://www.nltk.org/](https://www.nltk.org/)\n",
        "\n",
        "*   NLTK is a leading platform for building Python programs to work with human language data.\n",
        "*   It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet\n",
        "* text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '.venv (3.13.2) (Python 3.13.2)' requires the ipykernel package.\n",
            "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Users/Ehsan/Desktop/codes/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zH_5w2jSnMHg"
      },
      "outputs": [],
      "source": [
        "# !pip install nltk\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bchrWi-hnpC",
        "outputId": "31ac9068-7e7a-4f4f-94cc-eb9e38935e8c"
      },
      "outputs": [],
      "source": [
        "print (nltk.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvlFf8gmh2Oz",
        "outputId": "acdb5030-7b05-4a39-be99-0aebb3103ea5"
      },
      "outputs": [],
      "source": [
        "#PUNKT is a pre-trained unsupervised ML model that is a sentense tokenizer\n",
        "#Install PUNKT\n",
        "nltk.download ('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QYEjg91h9Er",
        "outputId": "9d30b798-fc0c-4670-d2c0-83110500712a"
      },
      "outputs": [],
      "source": [
        "#Sentence Tokenization\n",
        "from nltk.tokenize import sent_tokenize\n",
        "test_text = 'I learn NLP. I learn Python. Its user friendly. I am ready.'\n",
        "sent_tokenize(test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G-SYNWIiQlA",
        "outputId": "ba97bc43-3d02-4016-e6a4-5226e612c61b"
      },
      "outputs": [],
      "source": [
        "test2 = 'سلام! اسم من رضا هست. حالتون چطوره؟'\n",
        "sent_tokenize (test2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLsa3aDFlQSc",
        "outputId": "8440853f-bddc-4646-a437-a6cc5846a369"
      },
      "outputs": [],
      "source": [
        "!gdown --id 1oVyJvIIXM7eHBEMC_N-fxH9aaAUGiTL5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMmE-6xjkWl_"
      },
      "outputs": [],
      "source": [
        "#open a text file\n",
        "test_file = open(\"smaple_text.txt\", mode='r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_m2GLf5mWhj"
      },
      "source": [
        "### mode\n",
        "'r'\t: Open for text file for reading text \\\n",
        "'w'\t: Open a text file for writing text \\\n",
        "'a'\t: Open a text file for appending text\\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5NDRo4TmBgK",
        "outputId": "906eaa7c-6268-4b96-ad01-6aa4f706645f"
      },
      "outputs": [],
      "source": [
        "text_read = test_file.read()\n",
        "print(text_read)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eHvSukJmFP1",
        "outputId": "eb5f2781-b851-41c1-e509-5faf1be88311"
      },
      "outputs": [],
      "source": [
        "len(text_read) #the number of charachters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUIDOhZ5oyzy",
        "outputId": "a694b8e6-e4ee-4698-eed6-7542d96cda3d"
      },
      "outputs": [],
      "source": [
        "import nltk.data\n",
        "Punkt_tok = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
        "Punkt_tok.tokenize(text_read)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVoIEU4lpU1w",
        "outputId": "fb3bcd7f-deca-4f01-c4d0-e98c2dfd799b"
      },
      "outputs": [],
      "source": [
        "len(Punkt_tok.tokenize(text_read))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXvSLW_8rZfB"
      },
      "source": [
        "### We can train our tokenizer based on our text\n",
        "\n",
        "[Webtext (corpus)](https://paperswithcode.com/dataset/webtext) \\\n",
        "WebText is an internal OpenAI corpus created by scraping web pages with emphasis on document quality. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X9A9oH-reMQ",
        "outputId": "b2bb754b-c0fd-4aa0-ff9b-178820252dee"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('webtext')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72xenMzurigM",
        "outputId": "454c1147-e5be-481e-cf2c-93c2f70c1871"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import webtext\n",
        "text_parameter = webtext.raw('overheard.txt')\n",
        "print(text_parameter) # it is a play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6CLD-5wsW6d"
      },
      "outputs": [],
      "source": [
        "#Train my tokenizer\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "My_tokenizer = PunktSentenceTokenizer(text_parameter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaPhUO8Hsej_",
        "outputId": "b3af1b7e-11cd-4ebb-c8e6-5e3089e1903f"
      },
      "outputs": [],
      "source": [
        "type(My_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8qhiCQUshUF"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize    # to compare two methods\n",
        "pre_token = sent_tokenize(text_parameter)\n",
        "our_token = My_tokenizer.tokenize(text_parameter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "T823EuC-sn8T",
        "outputId": "2b87e511-3d25-47a9-c1ea-93587fdf9bb5"
      },
      "outputs": [],
      "source": [
        "pre_token[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FPxPJp1rsqZ9",
        "outputId": "c7559b53-6b80-4d59-e5cc-c15487baec91"
      },
      "outputs": [],
      "source": [
        "our_token[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_31tlkt7tqXI"
      },
      "source": [
        "## Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaDE-k4jtl1-",
        "outputId": "f01f691c-3642-48df-e127-f67d9bbef77a"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILUgmBtlty60",
        "outputId": "bc4ca345-87c1-4f7b-8009-5b4b5b2d33cc"
      },
      "outputs": [],
      "source": [
        "word_tokenize(\"don't\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpfRIeeIuHUF"
      },
      "source": [
        "### TreebankWordTokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4orzDZwbuIyp",
        "outputId": "c312e6f1-32e1-409b-9e1a-53ee51496ae9"
      },
      "outputs": [],
      "source": [
        "from nltk import TreebankWordTokenizer\n",
        "Tree_Toknizer = TreebankWordTokenizer()  # Create an object\n",
        "Tree_Toknizer.tokenize(\"Hello! Mr reza. How are you today? I can't stand\") # the same problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrZvYVVOuVFD"
      },
      "source": [
        "### WordPunktTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry6vR0zfuTzP",
        "outputId": "fc75569e-bc12-4a69-8bd1-6abc201274fb"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "Punkt_token = WordPunctTokenizer()\n",
        "Punkt_token.tokenize(\"can't\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Text-Introduction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.13.2)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
